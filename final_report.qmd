---
title: "Ad Fraud Detection: A Model Comparison Perspective"
author: Group3 - Daniel Ye, David Gao, Wanxin Tu, Yujie Zhao, Zihan Tang
format: html
editor: visual
---

Github repositoryo\<:https://github.com/amandazhao2022/STAT605_Project Kaggle data overview\<:https://www.kaggle.com/competitions/talkingdata-adtracking-fraud-detection/overview

## Introduction

To analyze today's online Ad fraud we used four models to predict whether a download of the app was made by a click, or the download request was sent by a malicious device. To analyze the data we first split the original file into 30 pieces, and then utilize the CHTC to run 30 parallel jobs to extract our training data by using the over sampling method. After we get the extractions, we merge them together as our training data, and then using CHTC to run models: logistics regression, Naive Bayes, support vector machine and random forest.

## Data Overview

We used 6 variables to build our models:

-   IP: IP address of click

-   app: app id for marketing

-   device:id of user mobile phone (e.g., iPhone, Samsung, etc.)

-   os: operating system version id of user mobile phone

-   channel: channel id of mobile ad publisher

-   is_attributed: the target that is to be predicted, indicating the app was downloaded

![Sample Data](https://raw.githubusercontent.com/amandazhao2022/STAT605_Project/main/691670476291_.pic.jpg){width="550"}

In a preliminary look at the data, we found the binary variable "is_attribute" has a skewed class proportion. As shown below, only a small proportion (less than 0.2%) of clicks were followed by a download, which indicates an imbalanced feature.

![Imbalanced Class](https://raw.githubusercontent.com/amandazhao2022/STAT605_Project/main/671670476291_.pic.jpg){width="550"}

## Data Preprocessing

To solve the imbalance feature, we first compared difference between oversampling and undersampling on the whole training file. For oversampling randomly, we duplicates observations from the minority class (value 1), set the number of minority to match that of the majority class (value 0); while for undersampling, we randomly removing observations from the majority and setting the number of samples to match that of the minority class. Finally, the processed file is merged into one, generating two cleaned data sets, one is oversampled, and the other is undersampled.

## Computation and Results

We shuffled the data and split 80% of the data for training, the rest 20% for prediction. Each data set is randomly split into four files of the same size and will be processed in parallel. They are used to examine four common prediction models:

1.  Logistic regression

The logistic regression estimates the coefficients in a linear combination, which is commonly used for prediction and classification.

2.  Random Forest

The random forest is a classification algorithm consisting of many decision trees. It takes each tree's majority vote for classification.

3.  Support vector machines

Support Vector Machine finds the hyperplane in an N-dimensional space that distinctly classifies the data points, it can be used for both classification and regression.

4.  Naive Bayes classifier

Naive Bayes assumes that each input variable is independent, which is one of the simple and most effective classification algorithms.

The rate of accuracy, precision, and recall are calculated to compare the effectiveness. Accuracy is the number of bots clicking the model correctly predicts divided by the total number of predictions. Precision is the proportion of bot clicking among those predicted as fraud. The higher the accuracy and the precision, the fewer false judgment made by the model. The recall is the proportion of correctly predicted bots clicking. The higher the recall, the more frauds are detected by the model.

As shown below, under both resampling methods, the Random Forest is the best among the four, with great comparative advantages in terms of accuracy, precision, and recall.

![](https://raw.githubusercontent.com/amandazhao2022/STAT605_Project/main/701670476291_.pic.jpg){width="550"}

![](https://raw.githubusercontent.com/amandazhao2022/STAT605_Project/main/661670476291_.pic.jpg){width="550"}

## Discussion

The project utilizes the potential of CHTC to handle large numbers of files in parallel, in the procession of both data cleaning and computation. It reduced the execution time and enhanced automation, thus improving efficiency.

![Process of Operation](https://raw.githubusercontent.com/amandazhao2022/STAT605_Project/main/681670476291_.pic.jpg){width="500"}

However, there do exist limitations. The highly unbalanced data means that any models we run on the data will either need to be robust against class imbalance or require data resampling. In this project, we performed both oversampling and undersampling. Oversampling the minority class leads to overfitting; the model learns patterns that only exist in the sample that has been oversampled. On the other hand, undersampling leads to underfitting, the model fails to capture the general pattern. Indeed, we decided to use oversampling as our primary data processing method as it will not lose information of the majority of the class.

## Conclusion

Random Forest is the best among the four models to predict whether a user will download an app after clicking a mobile app ad. Our project provided a reasonable basis for the advertising companies to develop a dynamic blacklist to prevent ad fraud. In this way, malicious bot activity can be filtered out, while real users are allowed to continue interacting.
